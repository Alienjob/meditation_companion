{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Assistant Feature Specification\n",
                "\n",
                "## 1. Purpose\n",
                "\n",
                "The Assistant feature provides a voice-based AI assistant that enables users to:\n",
                "- Have natural conversations with an LLM through voice and text\n",
                "- Interrupt the assistant mid-response when needed\n",
                "- See a synchronized chat transcript of the conversation\n",
                "- Resume conversation from the exact point of interruption\n",
                "\n",
                "This creates a more natural conversation experience, similar to talking with a human, where users can interrupt and redirect the conversation."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Key Functionality\n",
                "\n",
                "### Core Components\n",
                "\n",
                "1. **AssistantBloc**: Central coordinator managing state and events\n",
                "   - Handles OpenAI client connection and events\n",
                "   - Manages voice recording and playback states\n",
                "   - Coordinates with ChatBloc for message display\n",
                "\n",
                "2. **AudioService**: Manages audio playback with position tracking\n",
                "   - Streams audio chunks with item identification\n",
                "   - Provides interruption capability with position information\n",
                "   - Supports resuming from interruption point\n",
                "\n",
                "3. **PositionTracker**: Precise audio position tracking\n",
                "   - Tracks audio chunks and sample counts\n",
                "   - Accounts for pauses and buffering\n",
                "   - Provides accurate interruption positions\n",
                "\n",
                "### Key States\n",
                "\n",
                "```dart\n",
                "enum ClientStatus { connecting, ready, rateLimited, error }\n",
                "enum UserInputState { idle, recording, recorded }\n",
                "enum ResponseState { idle, responding }\n",
                "enum VoiceStreamState { idle, playing, error }\n",
                "```\n",
                "\n",
                "### Flow Overview\n",
                "\n",
                "1. **User Input Flow**:\n",
                "   - Record audio (up to 30 seconds)\n",
                "   - Send to OpenAI for transcription\n",
                "   - Display transcribed message in chat\n",
                "\n",
                "2. **Assistant Response Flow**:\n",
                "   - Receive text and audio chunks from OpenAI\n",
                "   - Display text in chat incrementally\n",
                "   - Stream audio for playback\n",
                "   - Track playback position for potential interruption\n",
                "\n",
                "3. **Interruption Flow**:\n",
                "   - User taps interrupt button\n",
                "   - Get current position (itemId + sampleCount) from PositionTracker\n",
                "   - Stops voice playback\n",
                "   - Send interruption to OpenAI with position\n",
                "   - Update UI to show interruption point\n",
                "   - Allow conversation to continue from that point\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Usage Examples\n",
                "\n",
                "### Setting up the AssistantBloc\n",
                "\n",
                "```dart\n",
                "class AssistantBloc extends Bloc<AssistantEvent, AssistantState> {\n",
                "  final ChatBloc _chatBloc;\n",
                "  final AudioService _audioService;\n",
                "  final AudioRecorder _recorder;\n",
                "  final RealtimeClient _client;\n",
                "  final PositionTracker _positionTracker;\n",
                "  \n",
                "  AssistantBloc({\n",
                "    required ChatBloc chatBloc,\n",
                "    required AudioService audioService,\n",
                "    required AudioRecorder recorder,\n",
                "  }) : _chatBloc = chatBloc,\n",
                "       _audioService = audioService,\n",
                "       _recorder = recorder,\n",
                "       _positionTracker = StreamTimeline(24000), // 24kHz for voice\n",
                "       _client = RealtimeClient(\n",
                "         apiKey: EnvConfig.openAiKey,\n",
                "         dangerouslyAllowAPIKeyInBrowser: true,\n",
                "       ) {\n",
                "    _setupClient();\n",
                "    _registerEventHandlers();\n",
                "  }\n",
                "  \n",
                "  Future<void> _setupClient() async {\n",
                "    // Initialize OpenAI client\n",
                "    await _client.updateSession(\n",
                "      instructions: 'You are a friendly meditation assistant.',\n",
                "      voice: Voice.alloy,\n",
                "    );\n",
                "    \n",
                "    // Register event handlers\n",
                "    _client.on(RealtimeEventType.conversationUpdated, _handleConversationUpdate);\n",
                "    _client.on(RealtimeEventType.conversationInterrupted, _handleInterruption);\n",
                "    \n",
                "    // Connect to OpenAI\n",
                "    await _client.connect();\n",
                "    add(ClientConnected());\n",
                "  }\n",
                "  \n",
                "  void _registerEventHandlers() {\n",
                "    on<StartRecordingUserAudioInput>(_onStartRecording);\n",
                "    on<StopRecordingUserAudioInput>(_onStopRecording);\n",
                "    on<SendRecordedAudio>(_onSendRecordedAudio);\n",
                "    on<InterruptResponse>(_onInterruptResponse);\n",
                "    // client.on(RealtimeEventType.conversationUpdated creates evenst \n",
                "    // Additional event handlers...\n",
                "  }\n",
                "}\n",
                "```\n",
                "\n",
                "### Handling Voice Recording and Sending\n",
                "\n",
                "```dart\n",
                "Future<void> _onStartRecording(\n",
                "  StartRecordingUserAudioInput event,\n",
                "  Emitter<AssistantState> emit,\n",
                ") async {\n",
                "  if (!state.canRecord) return;\n",
                "  \n",
                "  await _recorder.startRecording();\n",
                "  emit(state.copyWith(userInput: UserInputState.recording));\n",
                "  \n",
                "  // Start duration timer\n",
                "  _recordingTimer = Timer.periodic(\n",
                "    const Duration(milliseconds: 100),\n",
                "    (timer) {\n",
                "      final newDuration = state.recordingDuration + const Duration(milliseconds: 100);\n",
                "      if (newDuration >= AssistantState.maxRecordingDuration) {\n",
                "        add(StopRecordingUserAudioInput());\n",
                "      } else {\n",
                "        emit(state.copyWith(recordingDuration: newDuration));\n",
                "      }\n",
                "    },\n",
                "  );\n",
                "}\n",
                "\n",
                "Future<void> _onSendRecordedAudio(\n",
                "  SendRecordedAudio event,\n",
                "  Emitter<AssistantState> emit,\n",
                ") async {\n",
                "  if (!state.canSendRecording || state.recordedAudio == null) return;\n",
                "  \n",
                "  final base64Audio = base64Encode(state.recordedAudio!);\n",
                "  \n",
                "  // Send to OpenAI\n",
                "  await _client.sendUserMessageContent([\n",
                "    ContentPart.inputAudio(audio: base64Audio),\n",
                "  ]);\n",
                "  \n",
                "  // Reset recording state\n",
                "  emit(state.copyWith(\n",
                "    userInput: UserInputState.idle,\n",
                "    recordedAudio: null,\n",
                "    recordingDuration: Duration.zero,\n",
                "  ));\n",
                "}\n",
                "```\n",
                "\n",
                "### Processing Assistant Responses\n",
                "\n",
                "```dart\n",
                "void _handleConversationUpdate(RealtimeEvent event) {\n",
                "  final update = event as RealtimeEventConversationUpdated;\n",
                "  final result = update.result;\n",
                "  final item = result.item;\n",
                "  final delta = result.delta;\n",
                "  \n",
                "  if (item == null) return;\n",
                "  \n",
                "  // Handle user message transcription\n",
                "  if (item.item case ItemMessage message when message.role == ItemRole.user) {\n",
                "    if (delta?.transcript != null) {\n",
                "      add(UserMessageTranscribed(\n",
                "        itemId: item.id,\n",
                "        transcript: delta!.transcript!,\n",
                "      ));\n",
                "    }\n",
                "  }\n",
                "  \n",
                "  // Handle assistant response\n",
                "  if (item.item case ItemMessage message when message.role == ItemRole.assistant) {\n",
                "    // For text content\n",
                "    if (delta?.transcript != null) {\n",
                "      add(ResponseTextReceived(\n",
                "        itemId: item.id,\n",
                "        text: delta!.transcript!,\n",
                "      ));\n",
                "    }\n",
                "    \n",
                "    // For audio content\n",
                "    if (delta?.audio != null) {\n",
                "      final audioData = base64Decode(delta!.audio!);\n",
                "      add(ResponseAudioReceived(\n",
                "        itemId: item.id,\n",
                "        audioData: audioData,\n",
                "      ));\n",
                "    }\n",
                "  }\n",
                "}\n",
                "\n",
                "Future<void> _onResponseAudioReceived(\n",
                "  ResponseAudioReceived event,\n",
                "  Emitter<AssistantState> emit,\n",
                ") async {\n",
                "  // Add chunk to position tracker\n",
                "  _positionTracker.addChunk(event.itemId, event.audioData.length);\n",
                "  \n",
                "  // Send to audio service for playback\n",
                "  await _audioService.appendVoiceChunk(event.itemId, event.audioData);\n",
                "  \n",
                "  // Update state if not already responding\n",
                "  if (state.responseState != ResponseState.responding) {\n",
                "    emit(state.copyWith(responseState: ResponseState.responding));\n",
                "  }\n",
                "}\n",
                "```\n",
                "\n",
                "### Handling Interruptions\n",
                "\n",
                "```dart\n",
                "Future<void> _onInterruptResponse(\n",
                "  InterruptResponse event,\n",
                "  Emitter<AssistantState> emit,\n",
                ") async {\n",
                "  if (state.responseState != ResponseState.responding) return;\n",
                "  \n",
                "  // Get current position from tracker\n",
                "  final timestamp = DateTime.now().millisecondsSinceEpoch;\n",
                "  final result = _positionTracker.getInterruptionState(timestamp);\n",
                "  \n",
                "  // Stop audio playback\n",
                "  await _audioService.stopVoice();\n",
                "  \n",
                "  // Tell OpenAI about interruption\n",
                "  await _client.cancelResponse(result.itemId, result.sampleCount);\n",
                "  \n",
                "  // Update UI to show interruption\n",
                "  _chatBloc.add(MarkMessageAsInterrupted(\n",
                "    id: result.itemId,\n",
                "    interruptedAt: result.sampleCount,\n",
                "  ));\n",
                "  \n",
                "  // Update state\n",
                "  emit(state.copyWith(\n",
                "    responseState: ResponseState.interrupted,\n",
                "    lastInterruption: result,\n",
                "  ));\n",
                "  \n",
                "  // Reset trackers\n",
                "  _positionTracker.reset();\n",
                "}\n",
                "```\n",
                "\n",
                "### UI Integration Example\n",
                "\n",
                "```dart\n",
                "class AssistantView extends StatelessWidget {\n",
                "  @override\n",
                "  Widget build(BuildContext context) {\n",
                "    return BlocBuilder<AssistantBloc, AssistantState>(\n",
                "      builder: (context, state) {\n",
                "        return Column(\n",
                "          children: [\n",
                "            Expanded(\n",
                "              child: ChatWidget(), // Shows conversation history\n",
                "            ),\n",
                "            _buildControls(context, state),\n",
                "          ],\n",
                "        );\n",
                "      },\n",
                "    );\n",
                "  }\n",
                "  \n",
                "  Widget _buildControls(BuildContext context, AssistantState state) {\n",
                "    // Show recording controls\n",
                "    if (state.userInput == UserInputState.recording) {\n",
                "      return _buildRecordingControls(context, state);\n",
                "    }\n",
                "    \n",
                "    // Show send controls after recording\n",
                "    if (state.userInput == UserInputState.recorded) {\n",
                "      return _buildSendControls(context, state);\n",
                "    }\n",
                "    \n",
                "    // Show interrupt button during response\n",
                "    if (state.responseState == ResponseState.responding) {\n",
                "      return Center(\n",
                "        child: ElevatedButton.icon(\n",
                "          icon: const Icon(Icons.pan_tool),\n",
                "          label: const Text('Interrupt'),\n",
                "          onPressed: () => context.read<AssistantBloc>()\n",
                "              .add(const InterruptResponse()),\n",
                "          style: ElevatedButton.styleFrom(\n",
                "            backgroundColor: Colors.redAccent,\n",
                "          ),\n",
                "        ),\n",
                "      );\n",
                "    }\n",
                "    \n",
                "    // Default microphone button\n",
                "    return Center(\n",
                "      child: IconButton(\n",
                "        icon: const Icon(Icons.mic),\n",
                "        onPressed: state.canRecord\n",
                "            ? () => context.read<AssistantBloc>()\n",
                "                .add(const StartRecordingUserAudioInput())\n",
                "            : null,\n",
                "        iconSize: 48,\n",
                "      ),\n",
                "    );\n",
                "  }\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Test Case Descriptions\n",
                "\n",
                "### AssistantBloc Tests\n",
                "\n",
                "1. **Client Connection Tests**\n",
                "   - `should_initialize_client_successfully`: Verify client connects to OpenAI and emits proper state\n",
                "   - `should_handle_connection_failures`: Test error handling for connection issues\n",
                "   - `should_handle_rate_limiting`: Verify rate limit detection and appropriate UI feedback\n",
                "\n",
                "2. **Voice Recording Tests**\n",
                "   - `should_record_and_update_duration`: Test recording state and duration updates\n",
                "   - `should_stop_at_max_duration`: Verify recording stops at 30s limit\n",
                "   - `should_cancel_recording`: Test recording cancellation flow\n",
                "   - `should_send_recorded_audio`: Verify audio is properly encoded and sent\n",
                "\n",
                "3. **Response Handling Tests**\n",
                "   - `should_process_text_responses`: Test text chunks are passed to ChatBloc\n",
                "   - `should_process_audio_responses`: Verify audio is passed to AudioService\n",
                "   - `should_track_current_response_item`: Test response item ID tracking\n",
                "   - `should_handle_multiple_response_items`: Test handling of sequential responses\n",
                "\n",
                "4. **Interruption Tests**\n",
                "   - `should_interrupt_response_correctly`: Verify interruption process and position capture\n",
                "   - `should_send_proper_interruption_to_openai`: Test cancelResponse with correct parameters\n",
                "   - `should_mark_chat_message_as_interrupted`: Check chat message updating\n",
                "   - `should_update_ui_after_interruption`: Test UI state changes after interruption\n",
                "\n",
                "### AudioService Tests\n",
                "\n",
                "1. **Audio Streaming Tests**\n",
                "   - `should_handle_initial_chunk`: Test first chunk initialization\n",
                "   - `should_append_additional_chunks`: Verify multiple chunks are handled\n",
                "   - `should_handle_stream_completion`: Test proper completion state\n",
                "\n",
                "2. **Position Tracking Tests**\n",
                "   - `should_track_sample_count_accurately`: Verify sample counting logic\n",
                "   - `should_account_for_pauses`: Test pause handling in tracking\n",
                "   - `should_track_multiple_chunks`: Verify position across chunk boundaries\n",
                "\n",
                "3. **Stop Voice Tests**\n",
                "   - `should_stop_voice_correctly`: Verify playback stops\n",
                "   - `should_maintain_position_when_paused`: Test position stability during pauses\n",
                "   - `should_reset_after_completion`: Verify proper cleanup\n",
                "\n",
                "### UI Integration Tests\n",
                "\n",
                "1. **View Tests**\n",
                "   - `should_show_correct_controls_based_on_state`: Verify UI updates based on state\n",
                "   - `should_show_recording_progress`: Test recording progress indicator\n",
                "   - `should_disable_buttons_when_unavailable`: Check proper button state management\n",
                "\n",
                "2. **User Interaction Tests**\n",
                "   - `should_start_recording_on_mic_tap`: Verify recording starts\n",
                "   - `should_stop_recording_on_stop_tap`: Test recording stops\n",
                "   - `should_interrupt_on_button_press`: Verify interruption flow from UI\n",
                "\n",
                "3. **Chat Display Tests**\n",
                "   - `should_show_messages_with_proper_styling`: Test message appearance\n",
                "   - `should_show_interruption_markers`: Verify interruption UI indicators\n",
                "   - `should_scroll_to_new_messages`: Test auto-scrolling behavior\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
